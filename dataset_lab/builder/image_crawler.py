import os
import requests
import time
import zipfile
from pathlib import Path
from datetime import datetime
from urllib.parse import quote, urljoin
import hashlib
import json
import re

class BingImageCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def search_bing_images(self, keyword, max_images=10):
        """Bingç”»åƒæ¤œç´¢ã‹ã‚‰ç”»åƒURLã‚’å–å¾—ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        print(f"Bingã§ '{keyword}' ã‚’æ¤œç´¢ä¸­...")
        
        image_urls = []
        
        try:
            # Bingç”»åƒæ¤œç´¢ã®URLï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
            search_url = f"https://www.bing.com/images/search?q={quote(keyword)}&first=1"
            
            print(f"  ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {search_url}")
            
            # gzipåœ§ç¸®ã‚’æ˜ç¤ºçš„ã«å‡¦ç†
            response = self.session.get(search_url, timeout=15)
            response.raise_for_status()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒæ­£ã—ããƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            content = response.text
            
            # ãƒ‡ãƒãƒƒã‚°: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ä¸€éƒ¨ã‚’ç¢ºèª
            print(f"  ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚º: {len(content)} æ–‡å­—")
            
            # HTMLãŒæ­£ã—ãå–å¾—ã§ãã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if not content or len(content) < 1000 or '<html' not in content.lower():
                print(f"  âš  è­¦å‘Š: HTMLãŒæ­£ã—ãå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
                print(f"  Content-Encoding: {response.headers.get('Content-Encoding', 'ãªã—')}")
                print(f"  Content-Type: {response.headers.get('Content-Type', 'ãªã—')}")
                return []
            
            # æ­£è¦è¡¨ç¾ã§ç”»åƒURLã‚’æŠ½å‡º
            # æ–¹æ³•1: murl (é«˜è§£åƒåº¦ç”»åƒURL)
            pattern1 = r'"murl":"(https?://[^"]+)"'
            matches1 = re.findall(pattern1, content)
            
            # æ–¹æ³•2: turl (ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒURL)
            pattern2 = r'"turl":"(https?://[^"]+)"'
            matches2 = re.findall(pattern2, content)
            
            # æ–¹æ³•3: imgurl
            pattern3 = r'"imgurl":"(https?://[^"]+)"'
            matches3 = re.findall(pattern3, content)
            
            # æ–¹æ³•4: mediaurl
            pattern4 = r'"mediaurl":"(https?://[^"]+)"'
            matches4 = re.findall(pattern4, content)
            
            # ã™ã¹ã¦ã®URLã‚’çµåˆï¼ˆå„ªå…ˆé †ä½: murl > mediaurl > imgurl > turlï¼‰
            all_urls = matches1 + matches4 + matches3 + matches2
            
            print(f"  æ¤œå‡ºã•ã‚ŒãŸç”»åƒURLå€™è£œæ•°: {len(all_urls)}")
            
            # é‡è¤‡ã‚’é™¤å»ã—ã¦è¿½åŠ 
            seen = set()
            for url in all_urls:
                if url not in seen and url.startswith('http'):
                    # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—æ–‡å­—ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                    url = url.replace('\\u002f', '/').replace('\\/', '/').replace('\\u003d', '=').replace('\\u0026', '&')
                    
                    # æœ‰åŠ¹ãªç”»åƒURLã‹ãƒã‚§ãƒƒã‚¯
                    if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']) or 'image' in url.lower():
                        seen.add(url)
                        image_urls.append(url)
                        print(f"  [{len(image_urls)}/{max_images}] ç”»åƒURLã‚’å–å¾—")
                        
                        if len(image_urls) >= max_images:
                            break
            
            # ã¾ã è¶³ã‚Šãªã„å ´åˆã¯ã€ã‚ˆã‚Šåºƒç¯„å›²ã«æ¤œç´¢
            if len(image_urls) < max_images:
                print(f"  è¿½åŠ æ¤œç´¢ä¸­...")
                pattern_general = r'(https?://[^"\s\\]+\.(?:jpg|jpeg|png|gif|webp)(?:\?[^"\s\\]*)?)'
                matches_general = re.findall(pattern_general, content, re.IGNORECASE)
                
                for url in matches_general:
                    url_clean = url.replace('\\/', '/').replace('\\u002f', '/')
                    if url_clean not in seen and len(image_urls) < max_images:
                        seen.add(url_clean)
                        image_urls.append(url_clean)
                        print(f"  [{len(image_urls)}/{max_images}] ç”»åƒURLã‚’å–å¾—")
            
            if len(image_urls) == 0:
                print(f"  âš  ãƒ‡ãƒãƒƒã‚°: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æœ€åˆã®500æ–‡å­—:")
                print(f"  {content[:500]}")
            
            print(f"âœ“ {len(image_urls)} ä»¶ã®ç”»åƒURLã‚’å–å¾—ã—ã¾ã—ãŸ")
            return image_urls[:max_images]
            
        except Exception as e:
            print(f"âœ— æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def download_image(self, url, save_path, timeout=15):
        """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            # URLã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            url = url.replace('\\u002f', '/').replace('\\/', '/')
            
            response = self.session.get(url, timeout=timeout, stream=True)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆå°ã•ã™ãã‚‹ç”»åƒã¯å‰Šé™¤ï¼‰
            file_size = os.path.getsize(save_path)
            if file_size < 1024:  # 1KBæœªæº€
                os.remove(save_path)
                return False
            
            return True
            
        except Exception as e:
            if os.path.exists(save_path):
                os.remove(save_path)
            return False
    
    def get_file_extension(self, url):
        """URLã‹ã‚‰é©åˆ‡ãªæ‹¡å¼µå­ã‚’å–å¾—"""
        url_lower = url.lower()
        if '.jpg' in url_lower or '.jpeg' in url_lower:
            return 'jpg'
        elif '.png' in url_lower:
            return 'png'
        elif '.gif' in url_lower:
            return 'gif'
        elif '.webp' in url_lower:
            return 'webp'
        else:
            return 'jpg'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def crawl(self, keywords, max_images_per_keyword=10, output_dir="downloads"):
        """è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ç”»åƒã‚’ã‚¯ãƒ­ãƒ¼ãƒ«"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        temp_dir = Path(output_dir) / f"temp_{timestamp}"
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        total_downloaded = 0
        
        for keyword in keywords:
            print(f"\n{'='*60}")
            print(f"ğŸ“¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")
            print(f"{'='*60}")
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            safe_keyword = re.sub(r'[\\/*?:"<>|]', '_', keyword)
            keyword_dir = temp_dir / safe_keyword
            keyword_dir.mkdir(exist_ok=True)
            
            # ç”»åƒURLã‚’æ¤œç´¢
            image_urls = self.search_bing_images(keyword, max_images_per_keyword)
            
            if not image_urls:
                print(f"âš  '{keyword}' ã®ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                print(f"  ãƒ’ãƒ³ãƒˆ: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦ã¿ã¦ãã ã•ã„ï¼ˆä¾‹: 'ã‚¯ãƒ¯ã‚¬ã‚¿ æ˜†è™«'ï¼‰")
                continue
            
            # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            downloaded_count = 0
            for idx, url in enumerate(image_urls, 1):
                ext = self.get_file_extension(url)
                save_path = keyword_dir / f"{safe_keyword}_{idx:03d}.{ext}"
                
                print(f"  [{idx}/{len(image_urls)}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...", end=' ')
                
                if self.download_image(url, save_path):
                    print(f"âœ“ {save_path.name}")
                    downloaded_count += 1
                    total_downloaded += 1
                else:
                    print(f"âœ— ã‚¹ã‚­ãƒƒãƒ—")
                
                time.sleep(0.8)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            
            print(f"âœ“ {keyword}: {downloaded_count}/{len(image_urls)} æšãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        
        if total_downloaded == 0:
            print(f"\nâš  è­¦å‘Š: ç”»åƒãŒ1æšã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            print(f"  ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã™")
            import shutil
            shutil.rmtree(temp_dir)
            return None
        
        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        zip_filename = f"{output_dir}/bing_images_{timestamp}.zip"
        print(f"\n{'='*60}")
        print(f"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­: {zip_filename}")
        print(f"{'='*60}")
        
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in temp_dir.rglob('*'):
                if file_path.is_file():
                    arcname = file_path.relative_to(temp_dir)
                    zipf.write(file_path, arcname)
                    print(f"  è¿½åŠ : {arcname}")
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
        import shutil
        shutil.rmtree(temp_dir)
        
        print(f"\n{'='*60}")
        print(f"âœ… å®Œäº†!")
        print(f"{'='*60}")
        print(f"åˆè¨ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°: {total_downloaded} æš")
        print(f"ä¿å­˜å…ˆ: {zip_filename}")
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(zip_filename) / 1024 / 1024:.2f} MB")
        print(f"{'='*60}")
        
        return zip_filename


def main():
    print("="*60)
    print("ğŸ” Bingç”»åƒã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    print("="*60)
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
    print("\nğŸ’¡ ãƒ’ãƒ³ãƒˆ: è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›")
    print("   ä¾‹: ã‚¯ãƒ¯ã‚¬ã‚¿, ã‚«ãƒ–ãƒˆãƒ ã‚·, æ˜†è™«")
    keywords_input = input("\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›: ")
    keywords = [k.strip() for k in keywords_input.split(',') if k.strip()]
    
    if not keywords:
        print("âŒ ã‚¨ãƒ©ãƒ¼: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        return
    
    # ç”»åƒæšæ•°å…¥åŠ›
    try:
        max_images_input = input(f"å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã®ç”»åƒæšæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10): ").strip()
        max_images = int(max_images_input) if max_images_input else 10
    except ValueError:
        print("âš  ç„¡åŠ¹ãªæ•°å€¤ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤10ã‚’ä½¿ç”¨ã—ã¾ã™")
        max_images = 10
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = input("å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: downloads): ").strip() or "downloads"
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ è¨­å®šç¢ºèª")
    print(f"{'='*60}")
    print(f"  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords)}")
    print(f"  å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç”»åƒæšæ•°: {max_images}")
    print(f"  å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"{'='*60}\n")
    
    input("Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦é–‹å§‹...")
    
    # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œ
    crawler = BingImageCrawler()
    zip_file = crawler.crawl(keywords, max_images, output_dir)
    
    if zip_file:
        print(f"\nâœ¨ ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸ: {zip_file}")
        print(f"\nğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ–¹æ³•:")
        print(f"   1. å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ãƒ¼ã‹ã‚‰ '{zip_file}' ã‚’å³ã‚¯ãƒªãƒƒã‚¯")
        print(f"   2. 'Download...' ã‚’é¸æŠ")
    else:
        print(f"\nâŒ ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
        print(f"\nğŸ’¡ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:")
        print(f"   - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦ã¿ã¦ãã ã•ã„")
        print(f"   - ã‚ˆã‚Šå…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 'ã‚¯ãƒ¯ã‚¬ã‚¿ æ˜†è™«'ï¼‰")
        print(f"   - ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš  ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
