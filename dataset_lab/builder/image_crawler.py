import os
import requests
import time
import zipfile
from pathlib import Path
from datetime import datetime
from urllib.parse import quote, urljoin
import hashlib
import json
import re
from bs4 import BeautifulSoup

class BingImageCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def search_bing_images(self, keyword, max_images=10):
        """Bingç”»åƒæ¤œç´¢ã‹ã‚‰ç”»åƒURLã‚’å–å¾—"""
        print(f"Bingã§ '{keyword}' ã‚’æ¤œç´¢ä¸­...")
        
        image_urls = []
        count = 0
        
        try:
            # Bingç”»åƒæ¤œç´¢ã®URL
            search_url = f"https://www.bing.com/images/search?q={quote(keyword)}&form=HDRSC2&first=1&tsc=ImageBasicHover"
            
            response = self.session.get(search_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ–¹æ³•1: må±æ€§ã‹ã‚‰JSONã‚’æŠ½å‡º
            img_tags = soup.find_all('a', {'class': 'iusc'})
            
            for img_tag in img_tags:
                if count >= max_images:
                    break
                
                m_attr = img_tag.get('m')
                if m_attr:
                    try:
                        m_json = json.loads(m_attr)
                        img_url = m_json.get('murl') or m_json.get('turl')
                        
                        if img_url and img_url not in image_urls:
                            image_urls.append(img_url)
                            count += 1
                            print(f"  [{count}/{max_images}] ç”»åƒURLã‚’å–å¾—")
                    except json.JSONDecodeError:
                        continue
            
            # æ–¹æ³•2: imgã‚¿ã‚°ã‹ã‚‰ç›´æ¥å–å¾—ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
            if len(image_urls) < max_images:
                img_tags2 = soup.find_all('img', {'class': 'mimg'})
                for img in img_tags2:
                    if count >= max_images:
                        break
                    
                    img_url = img.get('src') or img.get('data-src')
                    if img_url and img_url.startswith('http') and img_url not in image_urls:
                        image_urls.append(img_url)
                        count += 1
                        print(f"  [{count}/{max_images}] ç”»åƒURLã‚’å–å¾—")
            
            print(f"âœ“ {len(image_urls)} ä»¶ã®ç”»åƒURLã‚’å–å¾—ã—ã¾ã—ãŸ")
            return image_urls[:max_images]
            
        except Exception as e:
            print(f"âœ— æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def download_image(self, url, save_path, timeout=15):
        """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            response = self.session.get(url, timeout=timeout, stream=True)
            response.raise_for_status()
            
            # Content-Typeã‹ã‚‰æ‹¡å¼µå­ã‚’åˆ¤å®š
            content_type = response.headers.get('content-type', '')
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆå°ã•ã™ãã‚‹ç”»åƒã¯å‰Šé™¤ï¼‰
            if os.path.getsize(save_path) < 1024:  # 1KBæœªæº€
                os.remove(save_path)
                return False
            
            return True
            
        except Exception as e:
            print(f"  âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            if os.path.exists(save_path):
                os.remove(save_path)
            return False
    
    def get_file_extension(self, url, content_type=''):
        """URLã¨Content-Typeã‹ã‚‰é©åˆ‡ãªæ‹¡å¼µå­ã‚’å–å¾—"""
        # URLã‹ã‚‰æ‹¡å¼µå­ã‚’æŠ½å‡º
        url_lower = url.lower()
        if '.jpg' in url_lower or '.jpeg' in url_lower or 'jpeg' in content_type:
            return 'jpg'
        elif '.png' in url_lower or 'png' in content_type:
            return 'png'
        elif '.gif' in url_lower or 'gif' in content_type:
            return 'gif'
        elif '.webp' in url_lower or 'webp' in content_type:
            return 'webp'
        else:
            return 'jpg'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def crawl(self, keywords, max_images_per_keyword=10, output_dir="downloads"):
        """è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ç”»åƒã‚’ã‚¯ãƒ­ãƒ¼ãƒ«"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        temp_dir = Path(output_dir) / f"temp_{timestamp}"
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        total_downloaded = 0
        
        for keyword in keywords:
            print(f"\n{'='*60}")
            print(f"ğŸ“¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")
            print(f"{'='*60}")
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            safe_keyword = re.sub(r'[\\/*?:"<>|]', '_', keyword)
            keyword_dir = temp_dir / safe_keyword
            keyword_dir.mkdir(exist_ok=True)
            
            # ç”»åƒURLã‚’æ¤œç´¢
            image_urls = self.search_bing_images(keyword, max_images_per_keyword)
            
            if not image_urls:
                print(f"âš  '{keyword}' ã®ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                continue
            
            # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            downloaded_count = 0
            for idx, url in enumerate(image_urls, 1):
                ext = self.get_file_extension(url)
                save_path = keyword_dir / f"{safe_keyword}_{idx:03d}.{ext}"
                
                print(f"  [{idx}/{len(image_urls)}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...", end=' ')
                
                if self.download_image(url, save_path):
                    print(f"âœ“ {save_path.name}")
                    downloaded_count += 1
                    total_downloaded += 1
                else:
                    print(f"âœ— ã‚¹ã‚­ãƒƒãƒ—")
                
                time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼ˆé‡è¦ï¼‰
            
            print(f"âœ“ {keyword}: {downloaded_count}/{len(image_urls)} æšãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        
        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        zip_filename = f"{output_dir}/bing_images_{timestamp}.zip"
        print(f"\n{'='*60}")
        print(f"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­: {zip_filename}")
        print(f"{'='*60}")
        
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in temp_dir.rglob('*'):
                if file_path.is_file():
                    arcname = file_path.relative_to(temp_dir)
                    zipf.write(file_path, arcname)
                    print(f"  è¿½åŠ : {arcname}")
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
        import shutil
        shutil.rmtree(temp_dir)
        
        print(f"\n{'='*60}")
        print(f"âœ… å®Œäº†!")
        print(f"{'='*60}")
        print(f"åˆè¨ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°: {total_downloaded} æš")
        print(f"ä¿å­˜å…ˆ: {zip_filename}")
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(zip_filename) / 1024 / 1024:.2f} MB")
        print(f"{'='*60}")
        
        return zip_filename


def main():
    print("="*60)
    print("ğŸ” Bingç”»åƒã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    print("="*60)
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
    print("\nğŸ’¡ ãƒ’ãƒ³ãƒˆ: è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›")
    keywords_input = input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›: ")
    keywords = [k.strip() for k in keywords_input.split(',') if k.strip()]
    
    if not keywords:
        print("âŒ ã‚¨ãƒ©ãƒ¼: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        return
    
    # ç”»åƒæšæ•°å…¥åŠ›
    try:
        max_images_input = input(f"å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã®ç”»åƒæšæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10): ").strip()
        max_images = int(max_images_input) if max_images_input else 10
    except ValueError:
        print("âš  ç„¡åŠ¹ãªæ•°å€¤ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤10ã‚’ä½¿ç”¨ã—ã¾ã™")
        max_images = 10
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = input("å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: downloads): ").strip() or "downloads"
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ è¨­å®šç¢ºèª")
    print(f"{'='*60}")
    print(f"  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords)}")
    print(f"  å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç”»åƒæšæ•°: {max_images}")
    print(f"  å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"{'='*60}\n")
    
    input("Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦é–‹å§‹...")
    
    # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œ
    crawler = BingImageCrawler()
    zip_file = crawler.crawl(keywords, max_images, output_dir)
    
    print(f"\nâœ¨ ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸ: {zip_file}")
    print(f"\nğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ–¹æ³•:")
    print(f"   1. å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ãƒ¼ã‹ã‚‰ '{zip_file}' ã‚’å³ã‚¯ãƒªãƒƒã‚¯")
    print(f"   2. 'Download...' ã‚’é¸æŠ")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš  ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
